---
title: "LinearRegression"
author: "Sam Milnes"
date: "4/3/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Add these stats to the 2022 Data
player	Driving Distance	sg_putt	ranking	rounds	gir	top_10	putts_round
Tiger	299.4	-0.634	18	23	0.6488	2	29.21
bubba	291.3	-0.309	66	12	0.6443	0	28.83
bryson	323.7	0.456	14	34	0.6715	9	28.64
rory	320.1	0.247	9	18	0.6451	2	28.28
Pieters	308.1	-0.451	34	8	0.5278	0	29.13




Loading in the data
```{r}

pga <- read.csv("C:\\Users\\milness\\Desktop\\Machine Learning\\Final Project\\2022Masters_pred_data.csv")

# Get rid of first column
pga <- pga[, -1]
# Get rid of x_attempts and hole proximity columns
pga <- subset(pga, select = -c(hole_proximity, x_of_attempts))
# Get rid of "T" in masters finish and ranking columns
pga$masters_finish<-gsub("T","",as.character(pga$masters_finish))
pga$ranking<-gsub("T","",as.character(pga$ranking))
# Converting distance columns to inches
pga_mat_fair <- stringr::str_extract_all(pga$prox_fair, "\\d+", simplify = TRUE)
pga$prox_fair <- as.numeric(pga_mat_fair[, 1]) * 12 + as.numeric(pga_mat_fair[, 2])
pga_mat_rough <- stringr::str_extract_all(pga$prox_rough, "\\d+", simplify = TRUE)
pga$prox_rough <- as.numeric(pga_mat_rough[, 1]) * 12 + as.numeric(pga_mat_rough[, 2])
pga_mat_arg <- stringr::str_extract_all(pga$prox_arg, "\\d+", simplify = TRUE)
pga$prox_arg <- as.numeric(pga_mat_arg[, 1]) * 12 + as.numeric(pga_mat_arg[, 2])


# Get rid of all rows that have "CU" for masters finish column. This indicates
# That the player did not make the cut, if kept in, this would skew the total score column
newPGA <- subset(pga, (pga$masters_finish != "CU"))
newPGA2 <- subset(newPGA, newPGA$masters_finish != "WD")
# Change the value in the masters finish column to 1 if "Win"
newPGA2$masters_finish[newPGA2$masters_finish == "Win"] <- 1
#Get rid of NA's
newPGA2 <- na.omit(newPGA2)
testData2021 <- newPGA2
# Drop player name column
newPGA2 <- newPGA2[, -2]
# Cleaned Data-frame to use for testing
#View(newPGA2)

```



Splitting the data into training and testing data
```{r}
# Getting all data before 2021 season
data.train <- newPGA2[which(newPGA2$year < 2021),]
data.train$ranking <- as.numeric(as.character(data.train$ranking))
#data.train$masters_finish <- as.numeric(as.character(data.train$masters_finish))
svm.data_train1 <- data.train
data.train <- subset(data.train, select = -c(masters_finish))
data.train <- data.train[, -1]


# Getting all data from the 2021 season.
data.test <- newPGA2[which(newPGA2$year == 2021),]
data.test$ranking <- as.numeric(as.character(data.test$ranking))

```



Preforming regsubsets on the data to find the best predictors. Here we can see
that the optimal number of predictors is 8 and they are listed below.

```{r}
library(leaps)
data.reg <- regsubsets(total_score ~., data = data.train)
data_reg.summary <- summary(data.reg)
#data_reg.summary
which.max(data_reg.summary$adjr2)
#plot(data_reg.summary$adjr2)
which.min(data_reg.summary$cp)
#plot(data_reg.summary$cp)
cbind(Cp = summary(data.reg)$cp,
  Adj_r2 = summary(data.reg)$adjr2)


data_reg.summary$which[8,]





```


Creating the linear regression model to predict total score
```{r}
masters.lin_reg <- lm(total_score ~ driving_distance + sg_putt +
                ranking + rounds + gir + top_10 + putts_round + gir_not_fair, data = data.train)
# summary(masters.lin_reg)
# Drop gir_not_fair due to high p-value
masters.lin_regFinal <- lm(total_score ~ driving_distance + sg_putt +
                ranking + rounds + gir + top_10 + putts_round,
data = data.train)

summary(masters.lin_regFinal)


```
Using anova to test if the model without gir_not_fair is better than the model
with it
H0: masters.lin_reg = masters.lin_regFinal
Ha: masters.lin_reg != masters.lin_regFinal
```{r}
library(car)

anova(masters.lin_regFinal, masters.lin_reg)

```
Here we can see that it does not make sense to keep the gir_not_fair predictor
because we get a very high p-value after we conduct the anova test. So our
final regression model will stay the same. So we fail to reject the null
hypothesis.



Preforming k-fold cross validation with the caret library.
```{r}
#install.packages("caret")
set.seed(8392)
library(caret)
ctrl <- trainControl(method = "cv", number = 10)
model.lm <- train(total_score ~ driving_distance + sg_putt +
    ranking + rounds + gir + top_10 +
      putts_round, data = data.train,
      method = "lm", trControl = ctrl)
print(model.lm)
model.lm$resample
```
We decided to use 10 folds in our cross validation. We ended up geting a root
mean squared error of 6.13353



Printing out the top ten finishers from the 2021 masters. I am going to use
this to my predicted top ten for the 2021 masters.
```{r}

#View(testData2021)
testData2021 <- testData2021[which(testData2021$year == 2021),]
testData2021$masters_finish <- sapply(testData2021$masters_finish, as.numeric)
testData2021$masters_top_10 <- ifelse(testData2021$masters_finish < 11, 1, 0)
#View(testData2021)
# Actual top 10
top10 <- testData2021[which(testData2021$masters_top_10 == 1),]
top10
top10Final <- subset(top10, select = c(player_name, masters_finish, total_score))
top10Final
View(top10Final)
top10Finishers <- subset(top10, select = c(player_name, masters_finish, total_score))
top10Finishers
```


Created the prediction function with my regression model and the test data set.
```{r}
prediction <- predict(masters.lin_regFinal, newdata = data.test)
#prediction

finalTestData <- testData2021
finalTestData$predictedFinalScore <- prediction
FinalTestDataSub <- subset(finalTestData, select =  c(year, player_name, total_score,
                                            masters_top_10, predictedFinalScore))

FinalTestDataSub[order(FinalTestDataSub$predictedFinalScore, decreasing = FALSE),]  

```


```{r}
predictionData <- read.csv("Masters2022FinalPredictionData.csv")
predictionData$ranking<-gsub("T","",as.character(predictionData$ranking))

predictionData$ranking <- as.numeric(as.character(predictionData$ranking))


prediction2022 <- predict(masters.lin_regFinal, newdata = predictionData)
#prediction2022


predictionData$PredictedTotal_Score <- prediction2022

FinalPredict <- subset(predictionData, select =  c(year, player_name, PredictedTotal_Score))

t69 <- rep(1:69)

top10new <- FinalPredict[order(FinalPredict$PredictedTotal_Score, decreasing = FALSE),] 
top10new$PredictFinish <- t69

top10Final <- head(top10new, 10)
t110 <- c(1,2,3,4,5,6,7,8,9,10)
top10Final$rank <- t110
top10Final <- top10Final[, -1]


```



SVM:

Adding column
1 if the golfer finished in the top 10 
0 if outside the top 10
```{r}


svm.data_train <- svm.data_train1
svm.data_test <- data.test

svm.data_test <- subset(svm.data_test, select = -c(year))
svm.data_train <- subset(svm.data_train, select = -c(year))

svm.data_train$masters_finish <- as.numeric(as.character(svm.data_train$masters_finish))
svm.data_train$masters_top_10 <- ifelse(svm.data_train$masters_finish < 11, 1, 0)

svm.data_test$masters_finish <- as.numeric(as.character(svm.data_test$masters_finish))
svm.data_test$masters_top_10 <- ifelse(svm.data_test$masters_finish < 11, 1, 0)
svm.data_test <- na.omit(svm.data_test)

# hist(svm.data_train$masters_top_10)

# svm.data_test <- subset(svm.data_test, select = -c(masters_finish))

# svm.data_test <- subset(svm.data_test, select = c(driving_distance, sg_putt, ranking, rounds, gir, top_10, putts_round, total_score))

```
Using Smote to work with unbalanced data

```{r}
#install.packages("smotefamily")
library(smotefamily)

smote <- SMOTE(svm.data_train[,-7], svm.data_train$masters_top_10)
new <- smote$data
#View(new)

hist(new$masters_top_10)
new$masters_top_10 <- as.factor(new$masters_top_10)
new$masters_top_10 <- as.factor(new$masters_top_10)

```




```{r}

library(e1071)
set.seed(1234)
svm.pga <- tune(svm, masters_top_10 ~ driving_distance + sg_putt + ranking +
                  rounds + gir + top_10 + putts_round,
                data = new,
                kernel = "linear",
                ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100)))
#summary(svm.auto)
bestmod = svm.pga$best.model
summary(svm.pga)

```


Polynomial SVM kernel
```{r}
set.seed(1234)
svm.pga_poly <- tune(svm, masters_top_10 ~ driving_distance + sg_putt + ranking +
                  rounds + gir + top_10 + putts_round,
                data = new,
                kernel = "polynomial",
                ranges = list(cost = c(0.1,1,10,100), degree = c(2, 3, 4, 5)))
#summary(svm.auto)
bestmod_poly = svm.pga_poly$best.model
summary(svm.pga_poly)

```

Radial SVM kernel
```{r}
set.seed(1234)
svm.pga_radial = tune(svm, masters_top_10 ~ driving_distance + sg_putt + ranking +
                  rounds + gir + top_10 + putts_round,
                data = new,
                kernel = "radial",
                ranges = list(cost = c(0.1,1,10,100), gamma = c(0.5,1,2,3)))
bestmod_radial = svm.pga_radial$best.model
summary(svm.pga_radial)

```






Testing SVM linear model
```{r}

ypred <- predict(bestmod, svm.data_test)
#ypred
table(predict = ypred, truth = svm.data_test$masters_top_10)

```

34 / 49 classified correctly
15 / 49 classified incorrectly -> 30.6% of observations were misclassified by 
this SVM model.



Testing SVM polynomial model
```{r}

ypredpoly <- predict(bestmod_poly, svm.data_test)
#ypredpoly
table(predict = ypredpoly, truth = svm.data_test$masters_top_10)

```


Testing SVM radial model
```{r}
ypredradial <- predict(bestmod_radial, svm.data_test)
#ypredradial
table(predict = ypredradial, truth = svm.data_test$masters_top_10)
```

Testing linear SVM Model on 2022 Data
```{r}

predictionDataSVM <- read.csv("Masters2022FinalPredictionData.csv")
predictionDataSVM$ranking<-gsub("T","",as.character(predictionDataSVM$ranking))

predictionDataSVM$ranking <- as.numeric(as.character(predictionDataSVM$ranking))

pred2022SVM <- predict(bestmod_poly, predictionDataSVM)
#pred2022SVM

predictionDataSVM$top10Finish <- pred2022SVM
FinalPredictSVM <- subset(predictionDataSVM, select =  c(year, player_name, top10Finish))

predicted_top_10 <- subset(FinalPredictSVM, FinalPredictSVM$top10Finish == 1)
predicted_top_10
# top10Final <- head(top10new, 10)
# t110 <- c(1,2,3,4,5,6,7,8,9,10)
# top10Final$rank <- t110
# top10Final <- top10Final[, -1]



```
Found that linear works best for training data, but polynomial works better for
new test data. Even though polynomial worked the best, it was only able
to narrow it down to 26 players, who it predicted would make top 10.













